{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About \u00b6 What is unblob? \u00b6 unblob is an accurate, fast, and easy-to-use extraction suite . It parses unknown binary blobs for more than 30 different archive, compression, and file-system formats , extracts their content recursively , and carves out unknown chunks that have not been accounted for. unblob is free to use , licensed under MIT license , it has a command line interface and can be used as a Python library. This turns unblob into the perfect companion for extracting, analyzing , and reverse engineering firmware images . unblob was originally developed and currently maintained by ONEKEY and it is used in production in our ONEKEY analysis platform . Demo \u00b6 Why unblob? \u00b6 One of the major challenges of embedded security analysis is the sound and safe extraction of arbitrary firmware. Specialized tools that can extract information from those firmware images already exist, but we were carving for something smarter that could identify both start-offset and end-offset of a specific chunk (e.g. filesystem, compression stream, archive, ...). We stick to the format standard as much as possible when deriving these offsets, and we clearly define what we want out of identified chunks (e.g., not extracting meta-data to disk, padding removal). This strategy helps us feed known valid data to extractors and precisely identify chunks, turning unknown unknowns into known unknowns. Given the modular design of unblob and the ever-expanding repository of supported formats , unblob could very well be used in areas outside embedded security such as data recovery, memory forensics, or malware analysis. Our Objectives \u00b6 unblob has been developed with the following objectives in mind: Accuracy - chunk start offsets are identified using battle tested rules, while end offsets are computed according to the format's standard without deviating from it. We minimize false positives as much as possible by validating header structures and discarding overlapping chunks. Security - unblob does not require elevated privileges to run. It's heavily tested and has been fuzz tested against a large corpus of files and firmware images. We rely on up-to-date third party dependencies that are locked to limit potential supply chain issues. We use safe extractors that we audited and fixed where required (see path traversal in ubi_reader , path traversal in jefferson , integer overflow in Yara ). Extensibility - unblob exposes an API that can be used to write custom format handlers and extractors in no time. Speed - we want unblob to be blazing fast, that's why we use multi-processing by default, make sure to write efficient code, use memory-mapped files, and use Hyperscan as a high-performance matching library. Computation-intensive functions are written in Rust and called from Python using specific bindings. How does it work? \u00b6 unblob identifies known and unknown chunks of data within a file: known chunks are identified by finding the start offset using a search rule, and the end offset is computed based on the format standard. Unknown chunks represents unidentified chunks of data before, after, or between known chunks. Unknown chunks composed of known content (e.g., null padding, 0xFF padding) are identified automatically and reported as such. unblob will carve out known chunks to disk and perform the extraction phase using the extractor assigned to a given handler. It will then walk the extracted content, looking for chunks in extracted files. a report on metadata can be generated by unblob, providing detailed information about identified chunks (format, offsets, size, entropy) and their extracted content if available (ownership, permissions, timestamps, ...). Used technologies \u00b6 unblob is written in Python . For quickly searching binary patterns in files, we use Hyperscan . For extracting recognized formats, we use all kinds of different Extractors . For ELF analysis, we are using LIEF with its Python bindings . For CPU-intensive tasks (e.g. entropy calculation), we use Rust to speed things up. For the pretty command line interface, we are using the Click library . For structured logging, we are using the structlog library . For development and testing tools, see the Development page. License \u00b6 unblob is licensed under the permissive MIT license , so you can use it without restrictions.","title":"About"},{"location":"#about","text":"","title":"About"},{"location":"#what-is-unblob","text":"unblob is an accurate, fast, and easy-to-use extraction suite . It parses unknown binary blobs for more than 30 different archive, compression, and file-system formats , extracts their content recursively , and carves out unknown chunks that have not been accounted for. unblob is free to use , licensed under MIT license , it has a command line interface and can be used as a Python library. This turns unblob into the perfect companion for extracting, analyzing , and reverse engineering firmware images . unblob was originally developed and currently maintained by ONEKEY and it is used in production in our ONEKEY analysis platform .","title":"What is unblob?"},{"location":"#demo","text":"","title":"Demo"},{"location":"#why-unblob","text":"One of the major challenges of embedded security analysis is the sound and safe extraction of arbitrary firmware. Specialized tools that can extract information from those firmware images already exist, but we were carving for something smarter that could identify both start-offset and end-offset of a specific chunk (e.g. filesystem, compression stream, archive, ...). We stick to the format standard as much as possible when deriving these offsets, and we clearly define what we want out of identified chunks (e.g., not extracting meta-data to disk, padding removal). This strategy helps us feed known valid data to extractors and precisely identify chunks, turning unknown unknowns into known unknowns. Given the modular design of unblob and the ever-expanding repository of supported formats , unblob could very well be used in areas outside embedded security such as data recovery, memory forensics, or malware analysis.","title":"Why unblob?"},{"location":"#our-objectives","text":"unblob has been developed with the following objectives in mind: Accuracy - chunk start offsets are identified using battle tested rules, while end offsets are computed according to the format's standard without deviating from it. We minimize false positives as much as possible by validating header structures and discarding overlapping chunks. Security - unblob does not require elevated privileges to run. It's heavily tested and has been fuzz tested against a large corpus of files and firmware images. We rely on up-to-date third party dependencies that are locked to limit potential supply chain issues. We use safe extractors that we audited and fixed where required (see path traversal in ubi_reader , path traversal in jefferson , integer overflow in Yara ). Extensibility - unblob exposes an API that can be used to write custom format handlers and extractors in no time. Speed - we want unblob to be blazing fast, that's why we use multi-processing by default, make sure to write efficient code, use memory-mapped files, and use Hyperscan as a high-performance matching library. Computation-intensive functions are written in Rust and called from Python using specific bindings.","title":"Our Objectives"},{"location":"#how-does-it-work","text":"unblob identifies known and unknown chunks of data within a file: known chunks are identified by finding the start offset using a search rule, and the end offset is computed based on the format standard. Unknown chunks represents unidentified chunks of data before, after, or between known chunks. Unknown chunks composed of known content (e.g., null padding, 0xFF padding) are identified automatically and reported as such. unblob will carve out known chunks to disk and perform the extraction phase using the extractor assigned to a given handler. It will then walk the extracted content, looking for chunks in extracted files. a report on metadata can be generated by unblob, providing detailed information about identified chunks (format, offsets, size, entropy) and their extracted content if available (ownership, permissions, timestamps, ...).","title":"How does it work?"},{"location":"#used-technologies","text":"unblob is written in Python . For quickly searching binary patterns in files, we use Hyperscan . For extracting recognized formats, we use all kinds of different Extractors . For ELF analysis, we are using LIEF with its Python bindings . For CPU-intensive tasks (e.g. entropy calculation), we use Rust to speed things up. For the pretty command line interface, we are using the Click library . For structured logging, we are using the structlog library . For development and testing tools, see the Development page.","title":"Used technologies"},{"location":"#license","text":"unblob is licensed under the permissive MIT license , so you can use it without restrictions.","title":"License"},{"location":"development/","text":"Development \u00b6 Want to contribute to unblob? That's great! We developed a framework (we sometimes reference it as \"unblob core\" ), to make it very easy to add support for new file formats . This page describes the process of how to do that. If you don't want or don't know how to develop complex Python applications, that's not a problem! If there is a format you would like to be supported in unblob and you can describe and explain it (maybe with nifty hex-representations, hand-drawings or smoke signs, or whatever you cup-of-tea is), we might help you implement it! Just open a new ticket in the GitHub issue tracker. If you do know all this stuff, and you have all the tools in the world installed, you can just jump to the How to write handlers section where the exciting stuff is. Setting up development environment \u00b6 Required tools \u00b6 Python : unblob requires Python 3.8 or above. Make sure that Python is installed on your system. git : You need it for cloning the repository. Install it from the git-scm website . Poetry : it is a package manager for Python dependencies. Follow the instructions on the Poetry website to install the latest version. pre-commit : We are using pre-commit to run checks like linters, type checks and formatting issues. Git LFS : We have big integration test files, and we are using Git LFS to track them. Install git-lfs from the website. Rust ( Optional ): unblob has an optional Rust extension for performance intensive processing. Building it is entirely optional and requires rustup to be installed on the host system. Follow the instructions on the rustup website to install it. pyenv ( Recommended ): When you are working with multiple versions of Python, pyenv makes it very easy to install and use different versions and make virtualenvs. Follow the instructions on GitHub for the installation. If your system already has at least Python 3.8 installed, you don't need it. Cloning the Git repository \u00b6 Set up your git config, fork the project on GitHub, then clone your fork locally. If you installed pre-commit , you can run pre-commit install , which makes pre-commit run automatically during git commits with git hooks, so you don't have to run them manually. You need to setup Git LFS once, before you will be able to run the whole test suite: git lfs install Warning If you have cloned the repository prior to installing Git LFS, you need to run the following commands in the cloned repository once: git lfs pull git lfs checkout Making a virtualenv \u00b6 The recommended way to develop Python projects in a semi-isolated way is to use virtualenv . If you don't want to manage it separately, you can rely on Poetry to automatically create a virtualenv for you on install. If you don't want Poetry to automatically create a virtualenv, you can turn it off with this command: poetry config virtualenvs.create false Or instead of Poetry you can use pyenv . You can set the Python interpreter version for the local folder only with: pyenv local 3.8.12 Installing Python dependencies \u00b6 We are using poetry to manage our Python dependencies. If you installed Rust: run UNBLOB_BUILD_RUST_EXTENSION=1 poetry install to build and install the extension. Set RUST_DEBUG=1 to build it in debug mode. poetry install will install all required dependencies for development. Running pre-commit \u00b6 If you installed the pre-commit git hook when setting up your local git repo, you don't need this step, otherwise you can run all checks with pre-commit run --all-files . Running the tests \u00b6 We are using pytest for running our test suite. We have big integration files in the tests/integration directory, we are using Git LFS to track them . Only after you installed Git LFS , can you run all tests, with python -m pytest tests/ in the activated virtualenv. Writing handlers \u00b6 Every handler inherits from the abstract class Handler located in unblob/models.py : class Handler ( abc . ABC ): \"\"\"A file type handler is responsible for searching, validating and \"unblobbing\" files from Blobs.\"\"\" NAME : str PATTERNS : str PATTERN_MATCH_OFFSET : int = 0 EXTRACTOR : Optional [ Extractor ] @classmethod def get_dependencies ( cls ): \"\"\"Returns external command dependencies needed for this handler to work.\"\"\" @abc . abstractmethod def calculate_chunk ( self , file : io . BufferedIOBase , start_offset : int ) -> Optional [ ValidChunk ]: \"\"\"Returns a ValidChunk when it found a valid format for this Handler. Otherwise it can raise and Exception or return None, those will be ignored. \"\"\" def extract ( self , inpath : Path , outdir : Path ): \"\"\"Responsible for extraction a ValidChunk.\"\"\" NAME : a unique name for this handler, this value will be appended at the end of carved out chunks PATTERNS : an array of Hyperscan rules. PATTERN_MATCH_OFFSET : an offset from the hyperscan match to the actual start offset. This happens when the magic is not the first field in a file header EXTRACTOR : an optional Extractor . It can be set to None if the handler is supposed to only carve files get_dependencies() : returns the extractor dependencies. This helps unblob keep track of third party dependencies . calculate_chunk() : this is the method that needs to be overridden in your handler. It receives a file object and the effective start_offset of your chunk. This is where you implement the logic to compute the end_offset and return a ValidChunk object. StructHandler class \u00b6 StructHandler is a specialized subclass of Handler that provides a structure parsing API based on the dissect.cstruct library: class StructHandler ( Handler ): C_DEFINITIONS : str HEADER_STRUCT : str def __init__ ( self ): self . _struct_parser = StructParser ( self . C_DEFINITIONS ) @property def cparser_le ( self ): return self . _struct_parser . cparser_le @property def cparser_be ( self ): return self . _struct_parser . cparser_be def parse_header ( self , file : io . BufferedIOBase , endian = Endian . LITTLE ): header = self . _struct_parser . parse ( self . HEADER_STRUCT , file , endian ) logger . debug ( \"Header parsed\" , header = header , _verbosity = 3 ) return header This class defines new attributes and methods: C_DEFINITIONS : a string holding one or multiple structures definitions in C, which will be used to parse the format. We use the following standard to define structs: typedef struct my_struct { uint8 header_length; } my_struct_t; HEADER_STRUCT : the name of your C structure that you'll use to parse the format header. parse_header() : it will parse the file from the current offset in endian endianness into a structure using HEADER_STRUCT defined in C_DEFINITIONS . If you need to parse structure using different endianness, the class exposes two properties: cparser_le : dissect.cstruct parser configured in little endian cparser_be : dissect.cstruct parser configured in big endian Recommendation If your format allows it, we strongly recommend you to inherit from the StructHandler given that it will be strongly typed and less prone to errors. Example Handler implementation \u00b6 Let's imagine that we have a custom file format that always starts with the magic: UNBLOB!! , followed by the size of the file (header included) as an unsigned 32 bit integer. First, we create a file in unblob/handlers/archive/myformat.py and write the skeleton of our handler: class MyformatHandler ( StructHandler ): NAME = \"myformat\" PATTERNS = [] C_DEFINITIONS = \"\" HEADER_STRUCT = \"\" EXTRACTOR = None def calculate_chunk ( self , file : io . BufferedIOBase , start_offset : int ) -> Optional [ ValidChunk ]: return We need to match on our custom magic. To find the right offset, we need to match on the UNBLOB!! byte pattern, so we add a HexString Hyperscan rule: class MyformatHandler ( StructHandler ): NAME = \"myformat\" PATTERNS = [ HexString ( \"55 4E 42 4C 4F 42 21 21\" ), # \"UNBLOB!!\" ] C_DEFINITIONS = \"\" HEADER_STRUCT = \"\" EXTRACTOR = None def calculate_chunk ( self , file : io . BufferedIOBase , start_offset : int ) -> Optional [ ValidChunk ]: return Then we need to parse the header, so we define a C structure in C_DEFINITIONS and adapt HEADER_STRUCT accordingly: class MyformatHandler ( StructHandler ): NAME = \"myformat\" PATTERNS = [ HexString ( \"55 4E 42 4C 4F 42 21 21\" ), # \"UNBLOB!!\" ] C_DEFINITIONS = r \"\"\" typedef struct myformat_header { char magic[8]; uint32 size; } myformat_header_t; \"\"\" HEADER_STRUCT = \"myformat_header_t\" EXTRACTOR = None def calculate_chunk ( self , file : io . BufferedIOBase , start_offset : int ) -> Optional [ ValidChunk ]: return With everything set, all that is left is to implement the calculate_chunk function: class MyformatHandler ( StructHandler ): NAME = \"myformat\" PATTERNS = [ HexString ( \"55 4E 42 4C 4F 42 21 21\" ), # \"UNBLOB!!\" ] C_DEFINITIONS = r \"\"\" typedef struct myformat_header { char magic[8]; uint32 size; } myformat_header_t; \"\"\" HEADER_STRUCT = \"myformat_header_t\" EXTRACTOR = None def calculate_chunk ( self , file : io . BufferedIOBase , start_offset : int ) -> Optional [ ValidChunk ]: header = self . parse_header ( file , Endian . LITTLE ) end_offset = start_offset + header . size return ValidChunk ( start_offset = start_offset , end_offset = end_offset ) That's it! Now you have a working handler for your own custom format! Testing Handlers \u00b6 If you want to submit a new format handler to unblob, it needs to come up with its own set of integration tests. We've implemented integration tests this way: pytest picks up integration test files corresponding to your handler in test/integration/type/handler_name/__input__ directory. pytest runs unblob on all the integration test files it picked up in the first step. pytest runs diff between the temporary extraction directory and test/integration/type/handler_name/__output__ . if no differences are observed the test pass, otherwise it fails. Important Create integration test files that cover all the possible scenarios of the target format . That includes different endianness, different versions, different padding, different algorithms. An excellent example of this is the integration test files for JFFS2 filesystems where we have filesystems covering both endianness (big endian, little endian), with or without padding, and with different compression algorithms (no compression, zlib, rtime, lzo): ./fruits.new.be.zlib.padded.jffs2 ./fruits.new.be.nocomp.padded.jffs2 ./fruits.new.be.rtime.jffs2 ./fruits.new.le.lzo.jffs2 ./fruits.new.le.rtime.jffs2 ./fruits.new.le.nocomp.padded.jffs2 ./fruits.new.be.rtime.padded.jffs2 ./fruits.new.be.lzo.jffs2 ./fruits.new.be.zlib.jffs2 ./fruits.new.le.zlib.padded.jffs2 ./fruits.new.be.lzo.padded.jffs2 ./fruits.new.le.lzo.padded.jffs2 ./fruits.new.be.nocomp.jffs2 ./fruits.new.le.zlib.jffs2 ./fruits.new.le.rtime.padded.jffs2 ./fruits.new.le.nocomp.jffs2 Utilities Functions \u00b6 We developed a bunch of utility functions which helped us during the development of existing unblob handlers. Do not hesitate to take a look at them in unblob/file_utils.py to see if any of those functions could help you during your own handler development. Hyperscan Rules \u00b6 Our hyperscan-based implementation accepts two different kinds of rule definitions: Regex and HexString . Regex \u00b6 This object simply represents any regular expression. Example: PATTERNS = [ Regex ( r \"-lh0-\" ) ] HexString \u00b6 This object can be used to write rules using the same DSL as Yara. The only limitation is that we do not support multi-line comments and unbounded jumps. Here's an example of a Hyperscan rule based on HexString : PATTERNS = [ HexString ( \"\"\" // this is a comment AA 00 [2] 01 \"\"\" ) ] Writing extractors \u00b6 Recommendation We support custom Python based extractors as part of unblob, but unless you write a handler for an exotic format, you should check if the Command extractor is sufficient for your needs, as it's very simple to use. Command extractor \u00b6 This extractor simply runs a command line tool on the carved-out file ( inpath ) to extract into the extraction directory ( outdir ). Below is the Command extractor instance of the ZIP handler: EXTRACTOR = Command ( \"7z\" , \"x\" , \"-p\" , \"-y\" , \" {inpath} \" , \"-o {outdir} \" ) If you have a custom format with no supported command to extract it, check out the Extractor Python class. Extractor class \u00b6 The Extractor interface is defined in unblob/models.py : class Extractor ( abc . ABC ): def get_dependencies ( self ) -> List [ str ]: \"\"\"Returns the external command dependencies.\"\"\" return [] @abc . abstractmethod def extract ( self , inpath : Path , outdir : Path ): \"\"\"Extract the carved out chunk. Raises ExtractError on failure.\"\"\" Two methods are exposed by this class: get_dependencies() : you should override it if your custom extractor relies on external dependencies such as command line tools extract() : you must override this function. This is where you'll perform the extraction of inpath content into outdir extraction directory Example Extractor \u00b6 Extractors are quite complex beasts, so rather than trying to come up with a fake example, we recommend you to read through our RomFS extractor code to see what it looks like in real world applications. Guidelines \u00b6 Code style \u00b6 We adhere to PEP8 and enforce proper formatting of source files using black so you should not worry about formatting source code at all, pre-commit will take care of it. File Format Correctness \u00b6 We want to strike the right balance between false positive reduction and a totally loose implementation. We tend not to validate checksums in order to still be able to extract corrupted content . However, if the lack of checksum validation gets in the way by leaving the handler generating a large amount of false positive, then it's time to revisit the handler and implement stronger header checks. Common unblob Handler Mistakes \u00b6 This is a collection of all the bad code we've seen during unblob development. Learn from us so you can avoid them in the future \ud83d\ude42 Use seek rather than read whenever possible, it's faster . You should always keep in mind to seek to the position the header starts or make sure you are always at the correct offset at all times. For example we made the mistake multiple times that read 4 bytes for file magic and didn't seek back. Watch out for negative seeking Make sure you get your types right! signedness can get in the way .","title":"Development"},{"location":"development/#development","text":"Want to contribute to unblob? That's great! We developed a framework (we sometimes reference it as \"unblob core\" ), to make it very easy to add support for new file formats . This page describes the process of how to do that. If you don't want or don't know how to develop complex Python applications, that's not a problem! If there is a format you would like to be supported in unblob and you can describe and explain it (maybe with nifty hex-representations, hand-drawings or smoke signs, or whatever you cup-of-tea is), we might help you implement it! Just open a new ticket in the GitHub issue tracker. If you do know all this stuff, and you have all the tools in the world installed, you can just jump to the How to write handlers section where the exciting stuff is.","title":"Development"},{"location":"development/#setting-up-development-environment","text":"","title":"Setting up development environment"},{"location":"development/#required-tools","text":"Python : unblob requires Python 3.8 or above. Make sure that Python is installed on your system. git : You need it for cloning the repository. Install it from the git-scm website . Poetry : it is a package manager for Python dependencies. Follow the instructions on the Poetry website to install the latest version. pre-commit : We are using pre-commit to run checks like linters, type checks and formatting issues. Git LFS : We have big integration test files, and we are using Git LFS to track them. Install git-lfs from the website. Rust ( Optional ): unblob has an optional Rust extension for performance intensive processing. Building it is entirely optional and requires rustup to be installed on the host system. Follow the instructions on the rustup website to install it. pyenv ( Recommended ): When you are working with multiple versions of Python, pyenv makes it very easy to install and use different versions and make virtualenvs. Follow the instructions on GitHub for the installation. If your system already has at least Python 3.8 installed, you don't need it.","title":"Required tools"},{"location":"development/#cloning-the-git-repository","text":"Set up your git config, fork the project on GitHub, then clone your fork locally. If you installed pre-commit , you can run pre-commit install , which makes pre-commit run automatically during git commits with git hooks, so you don't have to run them manually. You need to setup Git LFS once, before you will be able to run the whole test suite: git lfs install Warning If you have cloned the repository prior to installing Git LFS, you need to run the following commands in the cloned repository once: git lfs pull git lfs checkout","title":"Cloning the Git repository"},{"location":"development/#making-a-virtualenv","text":"The recommended way to develop Python projects in a semi-isolated way is to use virtualenv . If you don't want to manage it separately, you can rely on Poetry to automatically create a virtualenv for you on install. If you don't want Poetry to automatically create a virtualenv, you can turn it off with this command: poetry config virtualenvs.create false Or instead of Poetry you can use pyenv . You can set the Python interpreter version for the local folder only with: pyenv local 3.8.12","title":"Making a virtualenv"},{"location":"development/#installing-python-dependencies","text":"We are using poetry to manage our Python dependencies. If you installed Rust: run UNBLOB_BUILD_RUST_EXTENSION=1 poetry install to build and install the extension. Set RUST_DEBUG=1 to build it in debug mode. poetry install will install all required dependencies for development.","title":"Installing Python dependencies"},{"location":"development/#running-pre-commit","text":"If you installed the pre-commit git hook when setting up your local git repo, you don't need this step, otherwise you can run all checks with pre-commit run --all-files .","title":"Running pre-commit"},{"location":"development/#running-the-tests","text":"We are using pytest for running our test suite. We have big integration files in the tests/integration directory, we are using Git LFS to track them . Only after you installed Git LFS , can you run all tests, with python -m pytest tests/ in the activated virtualenv.","title":"Running the tests"},{"location":"development/#writing-handlers","text":"Every handler inherits from the abstract class Handler located in unblob/models.py : class Handler ( abc . ABC ): \"\"\"A file type handler is responsible for searching, validating and \"unblobbing\" files from Blobs.\"\"\" NAME : str PATTERNS : str PATTERN_MATCH_OFFSET : int = 0 EXTRACTOR : Optional [ Extractor ] @classmethod def get_dependencies ( cls ): \"\"\"Returns external command dependencies needed for this handler to work.\"\"\" @abc . abstractmethod def calculate_chunk ( self , file : io . BufferedIOBase , start_offset : int ) -> Optional [ ValidChunk ]: \"\"\"Returns a ValidChunk when it found a valid format for this Handler. Otherwise it can raise and Exception or return None, those will be ignored. \"\"\" def extract ( self , inpath : Path , outdir : Path ): \"\"\"Responsible for extraction a ValidChunk.\"\"\" NAME : a unique name for this handler, this value will be appended at the end of carved out chunks PATTERNS : an array of Hyperscan rules. PATTERN_MATCH_OFFSET : an offset from the hyperscan match to the actual start offset. This happens when the magic is not the first field in a file header EXTRACTOR : an optional Extractor . It can be set to None if the handler is supposed to only carve files get_dependencies() : returns the extractor dependencies. This helps unblob keep track of third party dependencies . calculate_chunk() : this is the method that needs to be overridden in your handler. It receives a file object and the effective start_offset of your chunk. This is where you implement the logic to compute the end_offset and return a ValidChunk object.","title":"Writing handlers"},{"location":"development/#structhandler-class","text":"StructHandler is a specialized subclass of Handler that provides a structure parsing API based on the dissect.cstruct library: class StructHandler ( Handler ): C_DEFINITIONS : str HEADER_STRUCT : str def __init__ ( self ): self . _struct_parser = StructParser ( self . C_DEFINITIONS ) @property def cparser_le ( self ): return self . _struct_parser . cparser_le @property def cparser_be ( self ): return self . _struct_parser . cparser_be def parse_header ( self , file : io . BufferedIOBase , endian = Endian . LITTLE ): header = self . _struct_parser . parse ( self . HEADER_STRUCT , file , endian ) logger . debug ( \"Header parsed\" , header = header , _verbosity = 3 ) return header This class defines new attributes and methods: C_DEFINITIONS : a string holding one or multiple structures definitions in C, which will be used to parse the format. We use the following standard to define structs: typedef struct my_struct { uint8 header_length; } my_struct_t; HEADER_STRUCT : the name of your C structure that you'll use to parse the format header. parse_header() : it will parse the file from the current offset in endian endianness into a structure using HEADER_STRUCT defined in C_DEFINITIONS . If you need to parse structure using different endianness, the class exposes two properties: cparser_le : dissect.cstruct parser configured in little endian cparser_be : dissect.cstruct parser configured in big endian Recommendation If your format allows it, we strongly recommend you to inherit from the StructHandler given that it will be strongly typed and less prone to errors.","title":"StructHandler class"},{"location":"development/#example-handler-implementation","text":"Let's imagine that we have a custom file format that always starts with the magic: UNBLOB!! , followed by the size of the file (header included) as an unsigned 32 bit integer. First, we create a file in unblob/handlers/archive/myformat.py and write the skeleton of our handler: class MyformatHandler ( StructHandler ): NAME = \"myformat\" PATTERNS = [] C_DEFINITIONS = \"\" HEADER_STRUCT = \"\" EXTRACTOR = None def calculate_chunk ( self , file : io . BufferedIOBase , start_offset : int ) -> Optional [ ValidChunk ]: return We need to match on our custom magic. To find the right offset, we need to match on the UNBLOB!! byte pattern, so we add a HexString Hyperscan rule: class MyformatHandler ( StructHandler ): NAME = \"myformat\" PATTERNS = [ HexString ( \"55 4E 42 4C 4F 42 21 21\" ), # \"UNBLOB!!\" ] C_DEFINITIONS = \"\" HEADER_STRUCT = \"\" EXTRACTOR = None def calculate_chunk ( self , file : io . BufferedIOBase , start_offset : int ) -> Optional [ ValidChunk ]: return Then we need to parse the header, so we define a C structure in C_DEFINITIONS and adapt HEADER_STRUCT accordingly: class MyformatHandler ( StructHandler ): NAME = \"myformat\" PATTERNS = [ HexString ( \"55 4E 42 4C 4F 42 21 21\" ), # \"UNBLOB!!\" ] C_DEFINITIONS = r \"\"\" typedef struct myformat_header { char magic[8]; uint32 size; } myformat_header_t; \"\"\" HEADER_STRUCT = \"myformat_header_t\" EXTRACTOR = None def calculate_chunk ( self , file : io . BufferedIOBase , start_offset : int ) -> Optional [ ValidChunk ]: return With everything set, all that is left is to implement the calculate_chunk function: class MyformatHandler ( StructHandler ): NAME = \"myformat\" PATTERNS = [ HexString ( \"55 4E 42 4C 4F 42 21 21\" ), # \"UNBLOB!!\" ] C_DEFINITIONS = r \"\"\" typedef struct myformat_header { char magic[8]; uint32 size; } myformat_header_t; \"\"\" HEADER_STRUCT = \"myformat_header_t\" EXTRACTOR = None def calculate_chunk ( self , file : io . BufferedIOBase , start_offset : int ) -> Optional [ ValidChunk ]: header = self . parse_header ( file , Endian . LITTLE ) end_offset = start_offset + header . size return ValidChunk ( start_offset = start_offset , end_offset = end_offset ) That's it! Now you have a working handler for your own custom format!","title":"Example Handler implementation"},{"location":"development/#testing-handlers","text":"If you want to submit a new format handler to unblob, it needs to come up with its own set of integration tests. We've implemented integration tests this way: pytest picks up integration test files corresponding to your handler in test/integration/type/handler_name/__input__ directory. pytest runs unblob on all the integration test files it picked up in the first step. pytest runs diff between the temporary extraction directory and test/integration/type/handler_name/__output__ . if no differences are observed the test pass, otherwise it fails. Important Create integration test files that cover all the possible scenarios of the target format . That includes different endianness, different versions, different padding, different algorithms. An excellent example of this is the integration test files for JFFS2 filesystems where we have filesystems covering both endianness (big endian, little endian), with or without padding, and with different compression algorithms (no compression, zlib, rtime, lzo): ./fruits.new.be.zlib.padded.jffs2 ./fruits.new.be.nocomp.padded.jffs2 ./fruits.new.be.rtime.jffs2 ./fruits.new.le.lzo.jffs2 ./fruits.new.le.rtime.jffs2 ./fruits.new.le.nocomp.padded.jffs2 ./fruits.new.be.rtime.padded.jffs2 ./fruits.new.be.lzo.jffs2 ./fruits.new.be.zlib.jffs2 ./fruits.new.le.zlib.padded.jffs2 ./fruits.new.be.lzo.padded.jffs2 ./fruits.new.le.lzo.padded.jffs2 ./fruits.new.be.nocomp.jffs2 ./fruits.new.le.zlib.jffs2 ./fruits.new.le.rtime.padded.jffs2 ./fruits.new.le.nocomp.jffs2","title":"Testing Handlers"},{"location":"development/#utilities-functions","text":"We developed a bunch of utility functions which helped us during the development of existing unblob handlers. Do not hesitate to take a look at them in unblob/file_utils.py to see if any of those functions could help you during your own handler development.","title":"Utilities Functions"},{"location":"development/#hyperscan-rules","text":"Our hyperscan-based implementation accepts two different kinds of rule definitions: Regex and HexString .","title":"Hyperscan Rules"},{"location":"development/#regex","text":"This object simply represents any regular expression. Example: PATTERNS = [ Regex ( r \"-lh0-\" ) ]","title":"Regex"},{"location":"development/#hexstring","text":"This object can be used to write rules using the same DSL as Yara. The only limitation is that we do not support multi-line comments and unbounded jumps. Here's an example of a Hyperscan rule based on HexString : PATTERNS = [ HexString ( \"\"\" // this is a comment AA 00 [2] 01 \"\"\" ) ]","title":"HexString"},{"location":"development/#writing-extractors","text":"Recommendation We support custom Python based extractors as part of unblob, but unless you write a handler for an exotic format, you should check if the Command extractor is sufficient for your needs, as it's very simple to use.","title":"Writing extractors"},{"location":"development/#command-extractor","text":"This extractor simply runs a command line tool on the carved-out file ( inpath ) to extract into the extraction directory ( outdir ). Below is the Command extractor instance of the ZIP handler: EXTRACTOR = Command ( \"7z\" , \"x\" , \"-p\" , \"-y\" , \" {inpath} \" , \"-o {outdir} \" ) If you have a custom format with no supported command to extract it, check out the Extractor Python class.","title":"Command extractor"},{"location":"development/#extractor-class","text":"The Extractor interface is defined in unblob/models.py : class Extractor ( abc . ABC ): def get_dependencies ( self ) -> List [ str ]: \"\"\"Returns the external command dependencies.\"\"\" return [] @abc . abstractmethod def extract ( self , inpath : Path , outdir : Path ): \"\"\"Extract the carved out chunk. Raises ExtractError on failure.\"\"\" Two methods are exposed by this class: get_dependencies() : you should override it if your custom extractor relies on external dependencies such as command line tools extract() : you must override this function. This is where you'll perform the extraction of inpath content into outdir extraction directory","title":"Extractor class"},{"location":"development/#example-extractor","text":"Extractors are quite complex beasts, so rather than trying to come up with a fake example, we recommend you to read through our RomFS extractor code to see what it looks like in real world applications.","title":"Example Extractor"},{"location":"development/#guidelines","text":"","title":"Guidelines"},{"location":"development/#code-style","text":"We adhere to PEP8 and enforce proper formatting of source files using black so you should not worry about formatting source code at all, pre-commit will take care of it.","title":"Code style"},{"location":"development/#file-format-correctness","text":"We want to strike the right balance between false positive reduction and a totally loose implementation. We tend not to validate checksums in order to still be able to extract corrupted content . However, if the lack of checksum validation gets in the way by leaving the handler generating a large amount of false positive, then it's time to revisit the handler and implement stronger header checks.","title":"File Format Correctness"},{"location":"development/#common-unblob-handler-mistakes","text":"This is a collection of all the bad code we've seen during unblob development. Learn from us so you can avoid them in the future \ud83d\ude42 Use seek rather than read whenever possible, it's faster . You should always keep in mind to seek to the position the header starts or make sure you are always at the correct offset at all times. For example we made the mistake multiple times that read 4 bytes for file magic and didn't seek back. Watch out for negative seeking Make sure you get your types right! signedness can get in the way .","title":"Common unblob Handler Mistakes"},{"location":"extractors/","text":"Extractors \u00b6 unblob relies on various tools for extracting the contents of a blob. These extractors are either third party tools (e.g. 7z) , or part of unblob (available in unblob/extractors directory or specific ones next to the handler, e.g.: unblob/handlers/filesystem/romfs.py ). To use unblob with all supported formats, all extractors need to be installed . See the Installation section on how to install extractors for various methods. You don't need to install any of these if you use Docker or Nix , as all extractors are included in those solutions. Checking installed extractors \u00b6 There is a --show-external-dependencies CLI option, which displays the name of the extractors used by unblob and shows if they are available for unblob to use or not: $ unblob --show-external-dependencies The following executables found installed, which are needed by unblob: 7z \u2713 debugfs \u2713 jefferson \u2713 lz4 \u2713 lziprecover \u2713 lzop \u2713 sasquatch \u2713 sasquatch-v4be \u2713 simg2img \u2713 ubireader_extract_files \u2713 ubireader_extract_images \u2713 unar \u2713 yaffshiv \u2713 zstd \u2713 NOTE : This option does NOT check the version of the extractors. Required extractors \u00b6 \u274c: If you installed unblob from source, you need to install these manually. \u2705: These extractors come with unblob, check pyproject.toml and poetry.lock for current versions. Extractor Provided commands Minimum version Pre-Installed More information p7zip-full 7z 16.02 \u274c https://www.7-zip.org/ e2fsprogs debugfs 1.45.5 \u274c http://e2fsprogs.sourceforge.net/ lz4 lz4 1.9.3 \u274c https://github.com/lz4/lz4 lziprecover lziprecover 1.22 \u274c http://www.nongnu.org/lzip/lziprecover.html lzop lzop 1.04 \u274c https://www.lzop.org/ android-sdk-libsparse-utils img2simg 8.1.0 \u274c https://packages.debian.org/unstable/android-sdk-libsparse-utils unar unar 1.10.1 \u274c https://theunarchiver.com/command-line sasquatch sasquatch , sasquatch-v4be 1.0 \u274c https://github.com/onekey-sec/sasquatch jefferson jefferson master \u2705 https://github.com/onekey-sec/jefferson ubireader ubireader_extract_files , ubireader_extract_images master \u2705 https://github.com/onekey-sec/ubi_reader yaffshiv yaffshiv master \u2705 https://github.com/onekey-sec/yaffshiv Maintained projects and forks \u00b6 We maintain a fork of several extractors, with many fixes and improvements. They are also available on GitHub: Jefferson for extracting JFFS2 is also a project of ONEKEY Fork of sasquatch based on squashfs-tools Fork of ubi_reader Python scripts for extracting UBI and UBIFS images Fork of yaffshiv a YAFFS file system parser and extractor","title":"Extractors"},{"location":"extractors/#extractors","text":"unblob relies on various tools for extracting the contents of a blob. These extractors are either third party tools (e.g. 7z) , or part of unblob (available in unblob/extractors directory or specific ones next to the handler, e.g.: unblob/handlers/filesystem/romfs.py ). To use unblob with all supported formats, all extractors need to be installed . See the Installation section on how to install extractors for various methods. You don't need to install any of these if you use Docker or Nix , as all extractors are included in those solutions.","title":"Extractors"},{"location":"extractors/#checking-installed-extractors","text":"There is a --show-external-dependencies CLI option, which displays the name of the extractors used by unblob and shows if they are available for unblob to use or not: $ unblob --show-external-dependencies The following executables found installed, which are needed by unblob: 7z \u2713 debugfs \u2713 jefferson \u2713 lz4 \u2713 lziprecover \u2713 lzop \u2713 sasquatch \u2713 sasquatch-v4be \u2713 simg2img \u2713 ubireader_extract_files \u2713 ubireader_extract_images \u2713 unar \u2713 yaffshiv \u2713 zstd \u2713 NOTE : This option does NOT check the version of the extractors.","title":"Checking installed extractors"},{"location":"extractors/#required-extractors","text":"\u274c: If you installed unblob from source, you need to install these manually. \u2705: These extractors come with unblob, check pyproject.toml and poetry.lock for current versions. Extractor Provided commands Minimum version Pre-Installed More information p7zip-full 7z 16.02 \u274c https://www.7-zip.org/ e2fsprogs debugfs 1.45.5 \u274c http://e2fsprogs.sourceforge.net/ lz4 lz4 1.9.3 \u274c https://github.com/lz4/lz4 lziprecover lziprecover 1.22 \u274c http://www.nongnu.org/lzip/lziprecover.html lzop lzop 1.04 \u274c https://www.lzop.org/ android-sdk-libsparse-utils img2simg 8.1.0 \u274c https://packages.debian.org/unstable/android-sdk-libsparse-utils unar unar 1.10.1 \u274c https://theunarchiver.com/command-line sasquatch sasquatch , sasquatch-v4be 1.0 \u274c https://github.com/onekey-sec/sasquatch jefferson jefferson master \u2705 https://github.com/onekey-sec/jefferson ubireader ubireader_extract_files , ubireader_extract_images master \u2705 https://github.com/onekey-sec/ubi_reader yaffshiv yaffshiv master \u2705 https://github.com/onekey-sec/yaffshiv","title":"Required extractors"},{"location":"extractors/#maintained-projects-and-forks","text":"We maintain a fork of several extractors, with many fixes and improvements. They are also available on GitHub: Jefferson for extracting JFFS2 is also a project of ONEKEY Fork of sasquatch based on squashfs-tools Fork of ubi_reader Python scripts for extracting UBI and UBIFS images Fork of yaffshiv a YAFFS file system parser and extractor","title":"Maintained projects and forks"},{"location":"formats/","text":"Supported file formats \u00b6 unblob supports more than 30 formats. You can see their code in unblob/handlers/ . \u2705: Some or all metadata is preserved for the format. \u274c: Metadata is not preserved (limitation of the format). Archives \u00b6 Format Preserved metadata Handler Extractor command timestamps permissions ownership AR \u274c \u274c \u274c archive/ar.py unar ARC \u274c \u274c \u274c archive/arc.py unar ARJ \u2705 \u2705 \u274c archive/arj.py 7z CAB \u274c \u274c \u274c archive/cab.py 7z CPIO \u2705 \u2705 \u2705 archive/cpio.py 7z DMG \u274c \u274c \u274c archive/dmg.py 7z RAR \u274c \u274c \u274c archive/rar.py unar 7ZIP \u274c \u274c \u274c archive/sevenzip.py 7z StuffIt \u274c \u274c \u274c archive/stuffit.py unar TAR \u2705 \u2705 \u2705 archive/tar.py 7z ZIP \u2705 \u2705 \u2705 archive/zip.py 7z Compression \u00b6 For compression formats, metadata cannot be preserved, as this information in most cases is not stored in the format. Format Handler Extractor bzip2 compression/bzip2.py 7z UNIX compress compression/compress.py 7z gzip compression/gzip.py 7z LZ4 compression/lz4.py lz4 LZH compression/lzh.py 7z LZIP compression/lzip.py lziprecover LZMA compression/lzma.py 7z LZO compression/lzo.py lzop XZ compression/xz.py 7z ZLIB compression/zlib.py ZlibExtractor custom code ZSTD compression/zstd.py zstd Filesystems \u00b6 Format Preserved metadata Handler Extractor command Android sparse image \u274c filesystem/android/sparse.py simg2img CRAMFS \u2705 filesystem/cramfs.py 7z ExtFS \u2705 filesystem/extfs.py debugfs FAT \u2705 filesystem/fat.py 7z ISO9660 \u2705 filesystem/iso9660.py 7z JFFS2 \u2705 filesystem/jffs2.py jefferson NTFS \u2705 filesystem/ntfs.py 7z RomFS \u2705 everything is o+rw or o+rwx filesystem/romfs.py RomFsExtractor custom code SquashFS (v3, v4) \u2705 filesystem/squashfs.py sasquatch SquashFS v4 Big Endian \u2705 filesystem/squashfs.py sasquatch-v4-be UBI \u2705 filesystem/ubi.py ubireader_extract_images UBIFS \u2705 filesystem/ubi.py ubireader_extract_files YAFFS (1, 2) \u2705 filesystem/yaffs.py yaffshiv Didn't find your format supported yet? \u00b6 unblob is easily extensible, and you can write your own handler and include your own extractors for proprietary formats. To learn more about this, see the development section . Alternatively, just open a new ticket in the Github issue tracker. Whenever we stumble upon proprietary formats in our ONEKEY analysis platform, we will add support for it. At this point, we have developed about a dozen of additional, proprietary format Handlers. If you are interested in a custom format not supported by the open source version, check out our platform at https://www.onekey.com or you can Contact Us .","title":"Supported Formats"},{"location":"formats/#supported-file-formats","text":"unblob supports more than 30 formats. You can see their code in unblob/handlers/ . \u2705: Some or all metadata is preserved for the format. \u274c: Metadata is not preserved (limitation of the format).","title":"Supported file formats"},{"location":"formats/#archives","text":"Format Preserved metadata Handler Extractor command timestamps permissions ownership AR \u274c \u274c \u274c archive/ar.py unar ARC \u274c \u274c \u274c archive/arc.py unar ARJ \u2705 \u2705 \u274c archive/arj.py 7z CAB \u274c \u274c \u274c archive/cab.py 7z CPIO \u2705 \u2705 \u2705 archive/cpio.py 7z DMG \u274c \u274c \u274c archive/dmg.py 7z RAR \u274c \u274c \u274c archive/rar.py unar 7ZIP \u274c \u274c \u274c archive/sevenzip.py 7z StuffIt \u274c \u274c \u274c archive/stuffit.py unar TAR \u2705 \u2705 \u2705 archive/tar.py 7z ZIP \u2705 \u2705 \u2705 archive/zip.py 7z","title":"Archives"},{"location":"formats/#compression","text":"For compression formats, metadata cannot be preserved, as this information in most cases is not stored in the format. Format Handler Extractor bzip2 compression/bzip2.py 7z UNIX compress compression/compress.py 7z gzip compression/gzip.py 7z LZ4 compression/lz4.py lz4 LZH compression/lzh.py 7z LZIP compression/lzip.py lziprecover LZMA compression/lzma.py 7z LZO compression/lzo.py lzop XZ compression/xz.py 7z ZLIB compression/zlib.py ZlibExtractor custom code ZSTD compression/zstd.py zstd","title":"Compression"},{"location":"formats/#filesystems","text":"Format Preserved metadata Handler Extractor command Android sparse image \u274c filesystem/android/sparse.py simg2img CRAMFS \u2705 filesystem/cramfs.py 7z ExtFS \u2705 filesystem/extfs.py debugfs FAT \u2705 filesystem/fat.py 7z ISO9660 \u2705 filesystem/iso9660.py 7z JFFS2 \u2705 filesystem/jffs2.py jefferson NTFS \u2705 filesystem/ntfs.py 7z RomFS \u2705 everything is o+rw or o+rwx filesystem/romfs.py RomFsExtractor custom code SquashFS (v3, v4) \u2705 filesystem/squashfs.py sasquatch SquashFS v4 Big Endian \u2705 filesystem/squashfs.py sasquatch-v4-be UBI \u2705 filesystem/ubi.py ubireader_extract_images UBIFS \u2705 filesystem/ubi.py ubireader_extract_files YAFFS (1, 2) \u2705 filesystem/yaffs.py yaffshiv","title":"Filesystems"},{"location":"formats/#didnt-find-your-format-supported-yet","text":"unblob is easily extensible, and you can write your own handler and include your own extractors for proprietary formats. To learn more about this, see the development section . Alternatively, just open a new ticket in the Github issue tracker. Whenever we stumble upon proprietary formats in our ONEKEY analysis platform, we will add support for it. At this point, we have developed about a dozen of additional, proprietary format Handlers. If you are interested in a custom format not supported by the open source version, check out our platform at https://www.onekey.com or you can Contact Us .","title":"Didn't find your format supported yet?"},{"location":"glossary/","text":"Glossary \u00b6 Handler \u00b6 a Handler in unblob is a Python class, which can detect and extract a specific file format. Extractor \u00b6 A tool, which can extract a file format, e.g. 7z , unrar , jefferson , etc. One tool might be used for multiple formats. Hyperscan \u00b6 A high-performance multiple regex matching library by Intel: https://www.hyperscan.io/ We are using it for finding specific bit/byte pattern matching like magic headers. Unknown chunk \u00b6 A byte stream which none of our Handler s was able to recognize. They are carved to separate files, with the filename including their start and end offsets. Valid chunk \u00b6 A ValidChunk is something that one of the Handler s found and we can extract. Recursion depth \u00b6 unblob is processing input files recursively, which means if we extracted a file, that contains further files inside it, those will also be extracted, until the recursion depth is reached. Beyond that level, no further extraction will happen. For example, if a tar.gz contains a zip and a text file, the recursion depth will be 3 : 1. gzip layer, 2. tar, 3. zip and text file.","title":"Glossary"},{"location":"glossary/#glossary","text":"","title":"Glossary"},{"location":"glossary/#handler","text":"a Handler in unblob is a Python class, which can detect and extract a specific file format.","title":"Handler"},{"location":"glossary/#extractor","text":"A tool, which can extract a file format, e.g. 7z , unrar , jefferson , etc. One tool might be used for multiple formats.","title":"Extractor"},{"location":"glossary/#hyperscan","text":"A high-performance multiple regex matching library by Intel: https://www.hyperscan.io/ We are using it for finding specific bit/byte pattern matching like magic headers.","title":"Hyperscan"},{"location":"glossary/#unknown-chunk","text":"A byte stream which none of our Handler s was able to recognize. They are carved to separate files, with the filename including their start and end offsets.","title":"Unknown chunk"},{"location":"glossary/#valid-chunk","text":"A ValidChunk is something that one of the Handler s found and we can extract.","title":"Valid chunk"},{"location":"glossary/#recursion-depth","text":"unblob is processing input files recursively, which means if we extracted a file, that contains further files inside it, those will also be extracted, until the recursion depth is reached. Beyond that level, no further extraction will happen. For example, if a tar.gz contains a zip and a text file, the recursion depth will be 3 : 1. gzip layer, 2. tar, 3. zip and text file.","title":"Recursion depth"},{"location":"guide/","text":"User guide \u00b6 Quickstart \u00b6 unblob has a very simple command line interface with sensible defaults . You just need to pass it a file you want to extract: $ unblob alpine-minirootfs-3.16.1-x86_64.tar.gz 2022-07-30 06:33.07 [info ] Start processing file file=openwrt-21.02.2-x86-64-generic-ext4-combined.img.gz pid=7092 It will make a new directory with the original filename appended with _extract : $ ls -l total 2656 drwxrwxr-x 3 walkman walkman 4096 Jul 30 08:43 alpine-minirootfs-3.16.1-x86_64.tar.gz_extract -rw-r--r-- 1 walkman walkman 2711958 Jul 30 08:43 alpine-minirootfs-3.16.1-x86_64.tar.gz And will extract all known file formats recursively until the specified recursion depth level (which is 10 by default): $ tree -L 2 alpine-minirootfs-3.16.1-x86_64.tar.gz_extract \u251c\u2500\u2500 alpine-minirootfs-3.16.1-x86_64.tar \u2514\u2500\u2500 alpine-minirootfs-3.16.1-x86_64.tar_extract \u251c\u2500\u2500 bin \u251c\u2500\u2500 dev \u251c\u2500\u2500 etc \u251c\u2500\u2500 home \u251c\u2500\u2500 lib \u251c\u2500\u2500 media \u251c\u2500\u2500 mnt \u251c\u2500\u2500 opt \u251c\u2500\u2500 proc \u251c\u2500\u2500 root \u251c\u2500\u2500 run \u251c\u2500\u2500 sbin \u251c\u2500\u2500 srv \u251c\u2500\u2500 sys \u251c\u2500\u2500 tmp \u251c\u2500\u2500 usr \u2514\u2500\u2500 var 18 directories, 1 file Features \u00b6 Metadata extraction \u00b6 unblob can generate a metadata file about the extracted files in a JSON format by using the --report CLI option: $ unblob --report alpine-report.json alpine-minirootfs-3.16.1-x86_64.tar.gz 2022-07-30 07:06.59 [info ] Start processing file file=alpine-minirootfs-3.16.1-x86_64.tar.gz pid=13586 2022-07-30 07:07.00 [info ] JSON report written path=alpine-report.json pid=13586 $ cat alpine-report.json [ { \"task\": { \"path\": \"/home/walkman/Projects/unblob/demo/alpine-minirootfs-3.16.1-x86_64.tar.gz\", \"depth\": 0, \"chunk_id\": \"\", \"__typename__\": \"Task\" }, \"reports\": [ { \"path\": \"/home/walkman/Projects/unblob/demo/alpine-minirootfs-3.16.1-x86_64.tar.gz\", \"size\": 2711958, \"is_dir\": false, \"is_file\": true, \"is_link\": false, \"link_target\": null, \"__typename__\": \"StatReport\" }, { \"magic\": \"gzip compressed data, max compression, from Unix, original size modulo 2^32 5816320\\\\012- data\", \"mime_type\": \"application/gzip\", \"__typename__\": \"FileMagicReport\" }, { \"id\": \"13590:1\", \"handler_name\": \"gzip\", \"start_offset\": 0, \"end_offset\": 2711958, \"size\": 2711958, \"is_encrypted\": false, \"extraction_reports\": [], \"__typename__\": \"ChunkReport\" } ], \"subtasks\": [ { \"path\": \"/home/walkman/Projects/unblob/demo/alpine-minirootfs-3.16.1-x86_64.tar.gz_extract\", \"depth\": 1, \"chunk_id\": \"13590:1\", \"__typename__\": \"Task\" } ], \"__typename__\": \"TaskResult\" }, ... ] Entropy calculation \u00b6 If you are analyzing an unknown file format, it might be useful to know the entropy of the contained files, so you can quickly see for example whether the file is encrypted or contains some random content. Let's make a file with fully random content at the start and end: $ dd if = /dev/random of = random1.bin bs = 10M count = 1 $ dd if = /dev/random of = random2.bin bs = 10M count = 1 $ cat random1.bin alpine-minirootfs-3.16.1-x86_64.tar.gz random2.bin > unknown-file A nice ASCII entropy plot is drawn on verbose level 3: $ unblob -vvv unknown-file | grep -C 15 \"Entropy distribution\" 2022-07-30 07:58.16 [debug ] Ended searching for chunks all_chunks=[0xa00000-0xc96196] pid=19803 2022-07-30 07:58.16 [debug ] Removed inner chunks outer_chunk_count=1 pid=19803 removed_inner_chunk_count=0 2022-07-30 07:58.16 [warning ] Found unknown Chunks chunks=[0x0-0xa00000, 0xc96196-0x1696196] pid=19803 2022-07-30 07:58.16 [info ] Extracting unknown chunk chunk=0x0-0xa00000 path=unknown-file_extract/0-10485760.unknown pid=19803 2022-07-30 07:58.16 [debug ] Carving chunk path=unknown-file_extract/0-10485760.unknown pid=19803 2022-07-30 07:58.16 [debug ] Calculating entropy for file path=unknown-file_extract/0-10485760.unknown pid=19803 size=0xa00000 2022-07-30 07:58.16 [debug ] Entropy calculated highest=99.99 lowest=99.98 mean=99.98 pid=19803 2022-07-30 07:58.16 [warning ] Drawing plot pid=19803 2022-07-30 07:58.16 [debug ] Entropy chart chart= Entropy distribution \u250c---------------------------------------------------------------------------\u2510 100\u2524\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2502 90\u2524 \u2502 80\u2524 \u2502 70\u2524 \u2502 60\u2524 \u2502 50\u2524 \u2502 40\u2524 \u2502 30\u2524 \u2502 20\u2524 \u2502 10\u2524 \u2502 0\u2524 \u2502 \u2514\u252c---\u252c---\u252c---\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c\u2518 1 4 7 12 16 20 24 29 33 37 41 46 50 54 59 63 67 71 76 80 [y] entropy % [x] mB pid=19803 2022-07-30 07:58.16 [info ] Extracting unknown chunk chunk=0xc96196-0x1696196 path=unknown-file_extract/13197718-23683478.unknown pid=19803 2022-07-30 07:58.16 [debug ] Carving chunk path=unknown-file_extract/13197718-23683478.unknown pid=19803 2022-07-30 07:58.16 [debug ] Calculating entropy for file path=unknown-file_extract/13197718-23683478.unknown pid=19803 size=0xa00000 2022-07-30 07:58.16 [debug ] Entropy calculated highest=99.99 lowest=99.98 mean=99.98 pid=19803 2022-07-30 07:58.16 [warning ] Drawing plot pid=19803 2022-07-30 07:58.16 [debug ] Entropy chart chart= Entropy distribution \u250c---------------------------------------------------------------------------\u2510 100\u2524\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2502 90\u2524 \u2502 80\u2524 \u2502 70\u2524 \u2502 60\u2524 \u2502 50\u2524 \u2502 40\u2524 \u2502 30\u2524 \u2502 20\u2524 \u2502 10\u2524 \u2502 0\u2524 \u2502 \u2514\u252c---\u252c---\u252c---\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c\u2518 1 4 7 12 16 20 24 29 33 37 41 46 50 54 59 63 67 71 76 80 [y] entropy % [x] mB Skip extraction with file magic \u00b6 The extraction process can be faster and produce fewer false positives if we just ignore some files, which we know will not contain meaningful results, or it makes no sense to extract them. Examples of such file formats are SQLite, images, fonts, or PDF documents. We have a default for the skip list , but you can change it with the --skip-magic CLI option. Here is a silly example: $ unblob --skip-magic \"POSIX tar archive\" alpine-minirootfs-3.16.1-x86_64.tar.gz 2022-07-30 07:18.09 [info ] Start processing file file=alpine-minirootfs-3.16.1-x86_64.tar.gz pid=14971 $ tree . \u251c\u2500\u2500 alpine-minirootfs-3.16.1-x86_64.tar.gz \u2514\u2500\u2500 alpine-minirootfs-3.16.1-x86_64.tar.gz_extract \u2514\u2500\u2500 alpine-minirootfs-3.16.1-x86_64.tar Here gzip has been extracted, but we skipped the tar extraction, so no other files have been extracted further. Full Command line interface \u00b6 Usage: unblob [OPTIONS] FILE A tool for getting information out of any kind of binary blob. You also need these extractor commands to be able to extract the supported file types: 7z, debugfs, jefferson, lz4, lziprecover, lzop, sasquatch, sasquatch-v4be, simg2img, ubireader_extract_files, ubireader_extract_images, unar, yaffshiv, zstd NOTE: Some older extractors might not be compatible. Options: -e, --extract-dir DIRECTORY Extract the files to this directory. Will be created if doesn't exist. -f, --force Force extraction even if outputs already exist (they are removed). -d, --depth INTEGER RANGE Recursion depth. How deep should we extract containers. [default: 10; x>=1] -n, --entropy-depth INTEGER RANGE Entropy calculation depth. How deep should we calculate entropy for unknown files? 1 means input files only, 0 turns it off. [default: 1; x>=0] -P, --plugins-path PATH Load plugins from the provided path. -S, --skip-magic TEXT Skip processing files with given magic prefix [default: BFLT, JPEG, GIF, PNG, SQLite, compiled Java class, TrueType Font data, PDF document, magic binary file, MS Windows icon resource, PE32+ executable (EFI application)] -p, --process-num INTEGER RANGE Number of worker processes to process files parallelly. [default: 12; x>=1] --report PATH File to store metadata generated during the extraction process (in JSON format). -k, --keep-extracted-chunks Keep extracted chunks -v, --verbose Verbosity level, counting, maximum level: 3 (use: -v, -vv, -vvv) --show-external-dependencies Shows commands needs to be available for unblob to work properly -h, --help Show this message and exit.","title":"User Guide"},{"location":"guide/#user-guide","text":"","title":"User guide"},{"location":"guide/#quickstart","text":"unblob has a very simple command line interface with sensible defaults . You just need to pass it a file you want to extract: $ unblob alpine-minirootfs-3.16.1-x86_64.tar.gz 2022-07-30 06:33.07 [info ] Start processing file file=openwrt-21.02.2-x86-64-generic-ext4-combined.img.gz pid=7092 It will make a new directory with the original filename appended with _extract : $ ls -l total 2656 drwxrwxr-x 3 walkman walkman 4096 Jul 30 08:43 alpine-minirootfs-3.16.1-x86_64.tar.gz_extract -rw-r--r-- 1 walkman walkman 2711958 Jul 30 08:43 alpine-minirootfs-3.16.1-x86_64.tar.gz And will extract all known file formats recursively until the specified recursion depth level (which is 10 by default): $ tree -L 2 alpine-minirootfs-3.16.1-x86_64.tar.gz_extract \u251c\u2500\u2500 alpine-minirootfs-3.16.1-x86_64.tar \u2514\u2500\u2500 alpine-minirootfs-3.16.1-x86_64.tar_extract \u251c\u2500\u2500 bin \u251c\u2500\u2500 dev \u251c\u2500\u2500 etc \u251c\u2500\u2500 home \u251c\u2500\u2500 lib \u251c\u2500\u2500 media \u251c\u2500\u2500 mnt \u251c\u2500\u2500 opt \u251c\u2500\u2500 proc \u251c\u2500\u2500 root \u251c\u2500\u2500 run \u251c\u2500\u2500 sbin \u251c\u2500\u2500 srv \u251c\u2500\u2500 sys \u251c\u2500\u2500 tmp \u251c\u2500\u2500 usr \u2514\u2500\u2500 var 18 directories, 1 file","title":"Quickstart"},{"location":"guide/#features","text":"","title":"Features"},{"location":"guide/#metadata-extraction","text":"unblob can generate a metadata file about the extracted files in a JSON format by using the --report CLI option: $ unblob --report alpine-report.json alpine-minirootfs-3.16.1-x86_64.tar.gz 2022-07-30 07:06.59 [info ] Start processing file file=alpine-minirootfs-3.16.1-x86_64.tar.gz pid=13586 2022-07-30 07:07.00 [info ] JSON report written path=alpine-report.json pid=13586 $ cat alpine-report.json [ { \"task\": { \"path\": \"/home/walkman/Projects/unblob/demo/alpine-minirootfs-3.16.1-x86_64.tar.gz\", \"depth\": 0, \"chunk_id\": \"\", \"__typename__\": \"Task\" }, \"reports\": [ { \"path\": \"/home/walkman/Projects/unblob/demo/alpine-minirootfs-3.16.1-x86_64.tar.gz\", \"size\": 2711958, \"is_dir\": false, \"is_file\": true, \"is_link\": false, \"link_target\": null, \"__typename__\": \"StatReport\" }, { \"magic\": \"gzip compressed data, max compression, from Unix, original size modulo 2^32 5816320\\\\012- data\", \"mime_type\": \"application/gzip\", \"__typename__\": \"FileMagicReport\" }, { \"id\": \"13590:1\", \"handler_name\": \"gzip\", \"start_offset\": 0, \"end_offset\": 2711958, \"size\": 2711958, \"is_encrypted\": false, \"extraction_reports\": [], \"__typename__\": \"ChunkReport\" } ], \"subtasks\": [ { \"path\": \"/home/walkman/Projects/unblob/demo/alpine-minirootfs-3.16.1-x86_64.tar.gz_extract\", \"depth\": 1, \"chunk_id\": \"13590:1\", \"__typename__\": \"Task\" } ], \"__typename__\": \"TaskResult\" }, ... ]","title":"Metadata extraction"},{"location":"guide/#entropy-calculation","text":"If you are analyzing an unknown file format, it might be useful to know the entropy of the contained files, so you can quickly see for example whether the file is encrypted or contains some random content. Let's make a file with fully random content at the start and end: $ dd if = /dev/random of = random1.bin bs = 10M count = 1 $ dd if = /dev/random of = random2.bin bs = 10M count = 1 $ cat random1.bin alpine-minirootfs-3.16.1-x86_64.tar.gz random2.bin > unknown-file A nice ASCII entropy plot is drawn on verbose level 3: $ unblob -vvv unknown-file | grep -C 15 \"Entropy distribution\" 2022-07-30 07:58.16 [debug ] Ended searching for chunks all_chunks=[0xa00000-0xc96196] pid=19803 2022-07-30 07:58.16 [debug ] Removed inner chunks outer_chunk_count=1 pid=19803 removed_inner_chunk_count=0 2022-07-30 07:58.16 [warning ] Found unknown Chunks chunks=[0x0-0xa00000, 0xc96196-0x1696196] pid=19803 2022-07-30 07:58.16 [info ] Extracting unknown chunk chunk=0x0-0xa00000 path=unknown-file_extract/0-10485760.unknown pid=19803 2022-07-30 07:58.16 [debug ] Carving chunk path=unknown-file_extract/0-10485760.unknown pid=19803 2022-07-30 07:58.16 [debug ] Calculating entropy for file path=unknown-file_extract/0-10485760.unknown pid=19803 size=0xa00000 2022-07-30 07:58.16 [debug ] Entropy calculated highest=99.99 lowest=99.98 mean=99.98 pid=19803 2022-07-30 07:58.16 [warning ] Drawing plot pid=19803 2022-07-30 07:58.16 [debug ] Entropy chart chart= Entropy distribution \u250c---------------------------------------------------------------------------\u2510 100\u2524\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2502 90\u2524 \u2502 80\u2524 \u2502 70\u2524 \u2502 60\u2524 \u2502 50\u2524 \u2502 40\u2524 \u2502 30\u2524 \u2502 20\u2524 \u2502 10\u2524 \u2502 0\u2524 \u2502 \u2514\u252c---\u252c---\u252c---\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c\u2518 1 4 7 12 16 20 24 29 33 37 41 46 50 54 59 63 67 71 76 80 [y] entropy % [x] mB pid=19803 2022-07-30 07:58.16 [info ] Extracting unknown chunk chunk=0xc96196-0x1696196 path=unknown-file_extract/13197718-23683478.unknown pid=19803 2022-07-30 07:58.16 [debug ] Carving chunk path=unknown-file_extract/13197718-23683478.unknown pid=19803 2022-07-30 07:58.16 [debug ] Calculating entropy for file path=unknown-file_extract/13197718-23683478.unknown pid=19803 size=0xa00000 2022-07-30 07:58.16 [debug ] Entropy calculated highest=99.99 lowest=99.98 mean=99.98 pid=19803 2022-07-30 07:58.16 [warning ] Drawing plot pid=19803 2022-07-30 07:58.16 [debug ] Entropy chart chart= Entropy distribution \u250c---------------------------------------------------------------------------\u2510 100\u2524\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2502 90\u2524 \u2502 80\u2524 \u2502 70\u2524 \u2502 60\u2524 \u2502 50\u2524 \u2502 40\u2524 \u2502 30\u2524 \u2502 20\u2524 \u2502 10\u2524 \u2502 0\u2524 \u2502 \u2514\u252c---\u252c---\u252c---\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c--\u2500\u252c\u2518 1 4 7 12 16 20 24 29 33 37 41 46 50 54 59 63 67 71 76 80 [y] entropy % [x] mB","title":"Entropy calculation"},{"location":"guide/#skip-extraction-with-file-magic","text":"The extraction process can be faster and produce fewer false positives if we just ignore some files, which we know will not contain meaningful results, or it makes no sense to extract them. Examples of such file formats are SQLite, images, fonts, or PDF documents. We have a default for the skip list , but you can change it with the --skip-magic CLI option. Here is a silly example: $ unblob --skip-magic \"POSIX tar archive\" alpine-minirootfs-3.16.1-x86_64.tar.gz 2022-07-30 07:18.09 [info ] Start processing file file=alpine-minirootfs-3.16.1-x86_64.tar.gz pid=14971 $ tree . \u251c\u2500\u2500 alpine-minirootfs-3.16.1-x86_64.tar.gz \u2514\u2500\u2500 alpine-minirootfs-3.16.1-x86_64.tar.gz_extract \u2514\u2500\u2500 alpine-minirootfs-3.16.1-x86_64.tar Here gzip has been extracted, but we skipped the tar extraction, so no other files have been extracted further.","title":"Skip extraction with file magic"},{"location":"guide/#full-command-line-interface","text":"Usage: unblob [OPTIONS] FILE A tool for getting information out of any kind of binary blob. You also need these extractor commands to be able to extract the supported file types: 7z, debugfs, jefferson, lz4, lziprecover, lzop, sasquatch, sasquatch-v4be, simg2img, ubireader_extract_files, ubireader_extract_images, unar, yaffshiv, zstd NOTE: Some older extractors might not be compatible. Options: -e, --extract-dir DIRECTORY Extract the files to this directory. Will be created if doesn't exist. -f, --force Force extraction even if outputs already exist (they are removed). -d, --depth INTEGER RANGE Recursion depth. How deep should we extract containers. [default: 10; x>=1] -n, --entropy-depth INTEGER RANGE Entropy calculation depth. How deep should we calculate entropy for unknown files? 1 means input files only, 0 turns it off. [default: 1; x>=0] -P, --plugins-path PATH Load plugins from the provided path. -S, --skip-magic TEXT Skip processing files with given magic prefix [default: BFLT, JPEG, GIF, PNG, SQLite, compiled Java class, TrueType Font data, PDF document, magic binary file, MS Windows icon resource, PE32+ executable (EFI application)] -p, --process-num INTEGER RANGE Number of worker processes to process files parallelly. [default: 12; x>=1] --report PATH File to store metadata generated during the extraction process (in JSON format). -k, --keep-extracted-chunks Keep extracted chunks -v, --verbose Verbosity level, counting, maximum level: 3 (use: -v, -vv, -vvv) --show-external-dependencies Shows commands needs to be available for unblob to work properly -h, --help Show this message and exit.","title":"Full Command line interface"},{"location":"installation/","text":"Installation \u00b6 Docker image \u00b6 unblob can be used right away from a docker image: ghcr.io/onekey-sec/unblob:latest , which contains everything needed to run unblob, even the extractors . The --pull always option is recommended, because the project is currently under heavy development, so we expect frequent changes. The extracted files will be in the /data/output folder inside the container. Mount your host directory where you want to see the extracted files there: docker run \\ --rm \\ --pull always \\ -v /path/to/extract-dir/on/host:/data/output \\ -v /path/to/files/on/host:/data/input \\ ghcr.io/onekey-sec/unblob:latest /data/input/path/to/file Help on usage: docker run --rm --pull always ghcr.io/onekey-sec/unblob:latest --help nix package \u00b6 unblob can be built and run using the Nix package manager. The Nix derivation installs all 3rd party dependencies. Install and configure Nix . Optional : enable the experimental features so that you don't need to pass --extra-experimental-features \"nix-command flakes\" to nix command invocations: cat > ~/.config/nix/nix.conf <<EOF experimental-features = nix-command flakes EOF Optional : use pre-built binaries from GitHub using cachix : nix-env -iA cachix -f https://cachix.org/api/v1/install cachix use unblob Install unblob: nix profile install github:onekey-sec/unblob Check that everything works correctly: unblob --show-external-dependencies From source \u00b6 Install Git if you don't have it yet. Install the Poetry Python package manager. Clone the unblob repository from GitHub : git clone https://github.com/onekey-sec/unblob.git Install Python dependencies with Poetry: Optional : With Rust optimizations (you need a Rust compiler ): cd unblob UNBLOB_BUILD_RUST_EXTENSION=1 poetry install --no-dev Python packages only: cd unblob poetry install --no-dev Install required extractors with your operating system package manager: on Ubuntu 22.04, install extractors with APT: sudo apt install e2fsprogs p7zip-full unar zlib1g-dev liblzo2-dev lzop lziprecover img2simg libhyperscan-dev zstd If you need squashfs support, install sasquatch: curl -L -o sasquatch_1.0_amd64.deb https://github.com/onekey-sec/sasquatch/releases/download/sasquatch-v1.0/sasquatch_1.0_amd64.deb sudo dpkg -i sasquatch_1.0_amd64.deb rm sasquatch_1.0_amd64.deb Check that everything works correctly: unblob --show-external-dependencies Dependencies are all in place: $ unblob --show-external-dependencies The following executables found installed, which are needed by unblob: 7z \u2713 debugfs \u2713 jefferson \u2713 lz4 \u2713 lziprecover \u2713 lzop \u2713 sasquatch \u2713 sasquatch-v4be \u2713 simg2img \u2713 ubireader_extract_files \u2713 ubireader_extract_images \u2713 unar \u2713 yaffshiv \u2713 zstd \u2713","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#docker-image","text":"unblob can be used right away from a docker image: ghcr.io/onekey-sec/unblob:latest , which contains everything needed to run unblob, even the extractors . The --pull always option is recommended, because the project is currently under heavy development, so we expect frequent changes. The extracted files will be in the /data/output folder inside the container. Mount your host directory where you want to see the extracted files there: docker run \\ --rm \\ --pull always \\ -v /path/to/extract-dir/on/host:/data/output \\ -v /path/to/files/on/host:/data/input \\ ghcr.io/onekey-sec/unblob:latest /data/input/path/to/file Help on usage: docker run --rm --pull always ghcr.io/onekey-sec/unblob:latest --help","title":"Docker image"},{"location":"installation/#nix-package","text":"unblob can be built and run using the Nix package manager. The Nix derivation installs all 3rd party dependencies. Install and configure Nix . Optional : enable the experimental features so that you don't need to pass --extra-experimental-features \"nix-command flakes\" to nix command invocations: cat > ~/.config/nix/nix.conf <<EOF experimental-features = nix-command flakes EOF Optional : use pre-built binaries from GitHub using cachix : nix-env -iA cachix -f https://cachix.org/api/v1/install cachix use unblob Install unblob: nix profile install github:onekey-sec/unblob Check that everything works correctly: unblob --show-external-dependencies","title":"nix package"},{"location":"installation/#from-source","text":"Install Git if you don't have it yet. Install the Poetry Python package manager. Clone the unblob repository from GitHub : git clone https://github.com/onekey-sec/unblob.git Install Python dependencies with Poetry: Optional : With Rust optimizations (you need a Rust compiler ): cd unblob UNBLOB_BUILD_RUST_EXTENSION=1 poetry install --no-dev Python packages only: cd unblob poetry install --no-dev Install required extractors with your operating system package manager: on Ubuntu 22.04, install extractors with APT: sudo apt install e2fsprogs p7zip-full unar zlib1g-dev liblzo2-dev lzop lziprecover img2simg libhyperscan-dev zstd If you need squashfs support, install sasquatch: curl -L -o sasquatch_1.0_amd64.deb https://github.com/onekey-sec/sasquatch/releases/download/sasquatch-v1.0/sasquatch_1.0_amd64.deb sudo dpkg -i sasquatch_1.0_amd64.deb rm sasquatch_1.0_amd64.deb Check that everything works correctly: unblob --show-external-dependencies Dependencies are all in place: $ unblob --show-external-dependencies The following executables found installed, which are needed by unblob: 7z \u2713 debugfs \u2713 jefferson \u2713 lz4 \u2713 lziprecover \u2713 lzop \u2713 sasquatch \u2713 sasquatch-v4be \u2713 simg2img \u2713 ubireader_extract_files \u2713 ubireader_extract_images \u2713 unar \u2713 yaffshiv \u2713 zstd \u2713","title":"From source"},{"location":"privacy/","text":"TLDR \u00b6 The website owner doesn't steal or sell any of your data. The website uses Google Analytics in a GDPR compliant way. Cookies are used where necessary. Local storage is used where necessary. The site is hosted by GitHub Pages. Introduction \u00b6 This policy sets out the different areas where user privacy is concerned and outlines the obligations & requirements of the users, the website and website owner. Furthermore the way this website processes, stores and protects user data and information will also be detailed within this policy. The Website \u00b6 This website and its owner take a proactive approach to user privacy and ensure the necessary steps are taken to protect the privacy of its users throughout their visiting experience. Access logs \u00b6 The service hosting the unblob.org website stores access logs. This data is not accessible to and not readable for the owner of this website. The hosting service, GitHub, has their privacy policy is available here: https://docs.github.com/en/github/site-policy/github-privacy-statement About cookies \u00b6 Cookie consent \u00b6 This website uses cookies. By using this website and agreeing to this policy, you consent to unblob.org 's use of cookies in accordance with the terms of this policy. Cookies are information packets sent by web servers to web browsers, and stored by the web browsers. The information is then sent back to the server each time the browser requests a page from the server. This enables a web server to identify and track web browsers. In order to comply with your preferences, the website may store a small number tiny functional cookies so that you're not asked to make this choice again, to be able to save your settings and to warn you of an updated changelog. Please note that these cookies are only used to guarantee a user-friendly visit and a proper website performance. Google Analytics \u00b6 unblob.org uses Google Analytics to analyse the use of this website. Google Analytics generates statistical and other information about website use by means of cookies. The information generated relating to this website is used to create reports about the use of the website. Google will store and use this information. Google's privacy policy is available at: https://www.google.com/policies/privacy . To opt-out Of Google Analytics Google created a browser add-on that allows you to turn off Google Analytics trackers, and works for Chrome, Firefox, Safari, Opera, and even Internet Explorer 11. Refusing cookies \u00b6 If you wish to disable cookies, you may do so through your individual browser options. More detailed information about cookie management with specific web browsers can be found at the browsers' respective websites. Local Storage \u00b6 Certain pages on this website may use your browser's local storage to allow you to store some history. This data is not available to the website owner, or sent to any third parties. External links \u00b6 Although this website only looks to include quality, safe and relevant external links users should always adopt a policy of caution before clicking any external web links mentioned throughout this website. The owner of this website cannot guarantee or verify the contents of any externally linked website despite his best efforts. Users should therefore note they click on external links at their own risk and this website and its owner cannot be held liable for any damages or implications caused by visiting any external links mentioned. Changes to policy \u00b6 Changes in this policy will be posted on this page. You are advised to check this page regularly to view the most recent privacy policy. Kudos \u00b6 This privacy policy was inspired by erresen.github.io","title":"Privacy Policy"},{"location":"privacy/#tldr","text":"The website owner doesn't steal or sell any of your data. The website uses Google Analytics in a GDPR compliant way. Cookies are used where necessary. Local storage is used where necessary. The site is hosted by GitHub Pages.","title":"TLDR"},{"location":"privacy/#introduction","text":"This policy sets out the different areas where user privacy is concerned and outlines the obligations & requirements of the users, the website and website owner. Furthermore the way this website processes, stores and protects user data and information will also be detailed within this policy.","title":"Introduction"},{"location":"privacy/#the-website","text":"This website and its owner take a proactive approach to user privacy and ensure the necessary steps are taken to protect the privacy of its users throughout their visiting experience.","title":"The Website"},{"location":"privacy/#access-logs","text":"The service hosting the unblob.org website stores access logs. This data is not accessible to and not readable for the owner of this website. The hosting service, GitHub, has their privacy policy is available here: https://docs.github.com/en/github/site-policy/github-privacy-statement","title":"Access logs"},{"location":"privacy/#about-cookies","text":"","title":"About cookies"},{"location":"privacy/#cookie-consent","text":"This website uses cookies. By using this website and agreeing to this policy, you consent to unblob.org 's use of cookies in accordance with the terms of this policy. Cookies are information packets sent by web servers to web browsers, and stored by the web browsers. The information is then sent back to the server each time the browser requests a page from the server. This enables a web server to identify and track web browsers. In order to comply with your preferences, the website may store a small number tiny functional cookies so that you're not asked to make this choice again, to be able to save your settings and to warn you of an updated changelog. Please note that these cookies are only used to guarantee a user-friendly visit and a proper website performance.","title":"Cookie consent"},{"location":"privacy/#google-analytics","text":"unblob.org uses Google Analytics to analyse the use of this website. Google Analytics generates statistical and other information about website use by means of cookies. The information generated relating to this website is used to create reports about the use of the website. Google will store and use this information. Google's privacy policy is available at: https://www.google.com/policies/privacy . To opt-out Of Google Analytics Google created a browser add-on that allows you to turn off Google Analytics trackers, and works for Chrome, Firefox, Safari, Opera, and even Internet Explorer 11.","title":"Google Analytics"},{"location":"privacy/#refusing-cookies","text":"If you wish to disable cookies, you may do so through your individual browser options. More detailed information about cookie management with specific web browsers can be found at the browsers' respective websites.","title":"Refusing cookies"},{"location":"privacy/#local-storage","text":"Certain pages on this website may use your browser's local storage to allow you to store some history. This data is not available to the website owner, or sent to any third parties.","title":"Local Storage"},{"location":"privacy/#external-links","text":"Although this website only looks to include quality, safe and relevant external links users should always adopt a policy of caution before clicking any external web links mentioned throughout this website. The owner of this website cannot guarantee or verify the contents of any externally linked website despite his best efforts. Users should therefore note they click on external links at their own risk and this website and its owner cannot be held liable for any damages or implications caused by visiting any external links mentioned.","title":"External links"},{"location":"privacy/#changes-to-policy","text":"Changes in this policy will be posted on this page. You are advised to check this page regularly to view the most recent privacy policy.","title":"Changes to policy"},{"location":"privacy/#kudos","text":"This privacy policy was inspired by erresen.github.io","title":"Kudos"},{"location":"support/","text":"Support \u00b6 Commercial support \u00b6 unblob is maintained by ONEKEY . For professional support or commercial enquiries, feel free to contact us via our website or write an email to support@onekey.com . If you want to use unblob within your research projects , don't hesitate to get in touch with us! The research team is directly reachable at research@onekey.com . Community support \u00b6 To report bugs , feature requests , odd errors or behaviors, or recommend other enhancements , do not hesitate to use our issue tracker . In absolutely no way should you open an issue in our issue tracker if you found a security issue within unblob. If you found one, please follow our security policy guidelines! If you have a question or want to start a longer discussion, or you are not sure how to use unblob or any other topic, you can open a Discussion on GitHub .","title":"Support"},{"location":"support/#support","text":"","title":"Support"},{"location":"support/#commercial-support","text":"unblob is maintained by ONEKEY . For professional support or commercial enquiries, feel free to contact us via our website or write an email to support@onekey.com . If you want to use unblob within your research projects , don't hesitate to get in touch with us! The research team is directly reachable at research@onekey.com .","title":"Commercial support"},{"location":"support/#community-support","text":"To report bugs , feature requests , odd errors or behaviors, or recommend other enhancements , do not hesitate to use our issue tracker . In absolutely no way should you open an issue in our issue tracker if you found a security issue within unblob. If you found one, please follow our security policy guidelines! If you have a question or want to start a longer discussion, or you are not sure how to use unblob or any other topic, you can open a Discussion on GitHub .","title":"Community support"}]}